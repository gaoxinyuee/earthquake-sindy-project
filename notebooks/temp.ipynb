{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Q_learning_agent:\n",
    "    \"\"\"Q Learning agent\n",
    "\n",
    "    Attributes:\n",
    "        env: OpenAI Gym environment\n",
    "        q: Q table\n",
    "        pos_space: position space\n",
    "        vel_space: velocity space\n",
    "        angle_space: pole angle space\n",
    "        angular_vel_space: pole angular velocity space\n",
    "\n",
    "        alpha: learning rate\n",
    "        gamma: discount factor\n",
    "        epsilon: exploration rate\n",
    "        epsilon_decay: exploration rate decay\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "\n",
    "        # Divide the continuous state space, position, velocity, pole angle, and pole angular velocity, into discrete bins using linspace\n",
    "        # These ranges are narrower than the environment's actual limits to keep bins manageable\n",
    "        self.pos_space = np.linspace(-2.4, 2.4, 7)\n",
    "        self.vel_space = np.linspace(-4.0, 4.0, 7)\n",
    "        self.angle_space = np.linspace(-.2095, .2095, 7)\n",
    "        self.angular_vel_space = np.linspace(-4.0, 4.0, 7)\n",
    "\n",
    "        # Initialize Q table\n",
    "        # Add +1 because np.digitize returns values from 0 to len(bins)\n",
    "        self.q = np.zeros((len(self.pos_space)+1, len(self.vel_space)+1, len(self.angle_space)+1, len(self.angular_vel_space)+1, self.env.action_space.n))\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.alpha = 0.1 # learning rate\n",
    "        self.gamma = 0.95 # discount factor\n",
    "        self.epsilon = 0.1 # exploration rate\n",
    "        self.epsilon_decay = 0.00001 # exploration rate decay\n",
    "\n",
    "        self.rng = np.random.default_rng() # Random number generator\n",
    "\n",
    "\n",
    "    def run_one_episode(self, is_training=True, render=False):\n",
    "        \"\"\"\n",
    "        Run one episode of the environment\n",
    "\n",
    "        Args:\n",
    "            is_training: boolean to indicate if the agent is training or testing\n",
    "        \"\"\"\n",
    "        # Reset environment and get initial state\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # Discretize the continuous state variables into bins using np.digitize\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        state_a = np.digitize(state[2], self.angle_space)\n",
    "        state_av = np.digitize(state[3], self.angular_vel_space)\n",
    "\n",
    "        # Initialize variables\n",
    "        terminated = False\n",
    "        rewards = 0\n",
    "\n",
    "        # Loop through one episode\n",
    "        while(not terminated):\n",
    "\n",
    "            if render:\n",
    "              self.env.render()\n",
    "\n",
    "            # Choose action based on epsilon-greedy policy: Explore or Exploit\n",
    "            if is_training and np.random.uniform() < self.epsilon:\n",
    "                action = self.env.action_space.sample() # Explore\n",
    "            else:\n",
    "                action = np.argmax(self.q[state_p, state_v, state_a, state_av, :]) # Exploit\n",
    "\n",
    "            # Choose action based on Q table\n",
    "\n",
    "\n",
    "            # Take action, get new state and reward\n",
    "            new_state, reward, terminated, _ = self.env.step(action)\n",
    "            new_state_p = np.digitize(new_state[0], self.pos_space)\n",
    "            new_state_v = np.digitize(new_state[1], self.vel_space)\n",
    "            new_state_a = np.digitize(new_state[2], self.angle_space)\n",
    "            new_state_av = np.digitize(new_state[3], self.angular_vel_space)\n",
    "            # Update Q table based on Bellman equation\n",
    "            if is_training:\n",
    "                self.q[state_p, state_v, state_a, state_av, action] = self.q[state_p, state_v, state_a, state_av, action] + self.alpha * (\n",
    "                    reward + self.gamma*np.max(self.q[new_state_p, new_state_v, new_state_a, new_state_av,:]) - self.q[state_p, state_v, state_a, state_av, action]\n",
    "                )\n",
    "            # Update to new state\n",
    "            state = new_state\n",
    "            state_p = new_state_p\n",
    "            state_v = new_state_v\n",
    "            state_a = new_state_a\n",
    "            state_av = new_state_av\n",
    "            rewards += reward\n",
    "        return rewards\n",
    "\n",
    "\n",
    "    def train(self, min_rewards=300, plot=False):\n",
    "        \"\"\"\n",
    "        Train the agent\n",
    "\n",
    "        Args:\n",
    "            min_rewards: minimum rewards to stop training\n",
    "            plot: boolean to indicate if the rewards should be plotted\n",
    "        \"\"\"\n",
    "        rewards_per_episode = []\n",
    "        i = 0\n",
    "\n",
    "        # Loop through episodes\n",
    "        while True:\n",
    "            rewards = self.run_one_episode() # Run one episode\n",
    "            rewards_per_episode.append(rewards) # Append rewards\n",
    "            mean_rewards = np.mean(rewards_per_episode[max(0, len(rewards_per_episode)-100):]) # Mean rewards of last 100 episodes\n",
    "            if i%100==0:\n",
    "                print(f'Episode: {i} Rewards: {rewards}  Epsilon: {self.epsilon:0.2f}  Mean Rewards {mean_rewards:0.1f}')\n",
    "            if mean_rewards > min_rewards:\n",
    "                break\n",
    "            self.epsilon = max(self.epsilon - self.epsilon_decay, 0.1) # Decay epsilon\n",
    "            i += 1\n",
    "        self.env.close()\n",
    "\n",
    "        # plot the rewards\n",
    "        if plot:\n",
    "            self.plot_rewards(rewards_per_episode)\n",
    "\n",
    "\n",
    "    def train_by_episodes(self, num_episodes=1000, plot=False):\n",
    "        \"\"\"\n",
    "        Train the agent by episodes\n",
    "\n",
    "        Args:\n",
    "            num_episodes: number of episodes to train\n",
    "            plot: boolean to indicate if the rewards should be plotted\n",
    "        \"\"\"\n",
    "        rewards_per_episode = []\n",
    "        for i in range(num_episodes):\n",
    "            rewards = self.run_one_episode()\n",
    "            rewards_per_episode.append(rewards)\n",
    "            mean_rewards = np.mean(rewards_per_episode[max(0, len(rewards_per_episode)-100):])\n",
    "            if i%100==0:\n",
    "                print(f'Episode: {i} Rewards: {rewards}  Epsilon: {self.epsilon:0.2f}  Mean Rewards {mean_rewards:0.1f}')\n",
    "            self.epsilon = max(self.epsilon - self.epsilon_decay, 0.01)\n",
    "        self.env.close()\n",
    "\n",
    "        # plot the rewards\n",
    "        if plot:\n",
    "            self.plot_rewards(rewards_per_episode)\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, num_episodes=100, plot=True):\n",
    "        \"\"\"\n",
    "        Test the agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes: number of episodes to test\n",
    "            plot: boolean to indicate if the rewards should be plotted\n",
    "        \"\"\"\n",
    "        rewards_per_episode = []\n",
    "        for i in range(num_episodes):\n",
    "            rewards = self.run_one_episode(is_training=False) # Run one episode\n",
    "            rewards_per_episode.append(rewards) # Append rewards\n",
    "            print(f'Episode: {i+1} Rewards: {rewards}')\n",
    "        print(f'Cummulative average rewards: {np.mean(rewards_per_episode)}')\n",
    "\n",
    "        # plot the rewards\n",
    "        if plot:\n",
    "            plt.plot(rewards_per_episode)\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.ylabel('Rewards')\n",
    "            plt.title(f'Rewards vs Episodes)')\n",
    "            plt.savefig('cartpoleQLAgent_test.png')\n",
    "\n",
    "        return np.mean(rewards_per_episode)\n",
    "\n",
    "\n",
    "    def show_action(self):\n",
    "        \"\"\"\n",
    "        Show the action\n",
    "\n",
    "        \"\"\"\n",
    "        # Reset and get initial state\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # Discretize state\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        state_a = np.digitize(state[2], self.angle_space)\n",
    "        state_av = np.digitize(state[3], self.angular_vel_space)\n",
    "\n",
    "        # Choose action based on Q table\n",
    "        action = np.argmax(self.q[state_p, state_v, state_a, state_av, :])\n",
    "\n",
    "        # Return state and action\n",
    "        return state, action\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Predict the action\n",
    "\n",
    "        Args:\n",
    "            state: state to predict the action\n",
    "        \"\"\"\n",
    "        # Discretize state\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        state_a = np.digitize(state[2], self.angle_space)\n",
    "        state_av = np.digitize(state[3], self.angular_vel_space)\n",
    "        action = np.argmax(self.q[state_p, state_v, state_a, state_av, :])\n",
    "        return action\n",
    "\n",
    "\n",
    "    #Saves model to preserve the state of QL agent after training\n",
    "    def save_model(self, filename='cartpoleQLAgent.pkl'):\n",
    "        \"\"\"\n",
    "        Save and load the model\n",
    "\n",
    "        Args:\n",
    "            filename: name of the file to save/load the model\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.q, f)\n",
    "\n",
    "\n",
    "    #Loads a previously saved Q-table from a file\n",
    "    def load_model(self, filename='cartpoleQLAgent.pkl'):\n",
    "        \"\"\"\n",
    "        Load the model\n",
    "\n",
    "        Args:\n",
    "            filename: name of the file to load the model\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.q = pickle.load(f)\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards_array):\n",
    "        \"\"\"\n",
    "        Plot the rewards\n",
    "\n",
    "        Args:\n",
    "            rewards_array: array of rewards\n",
    "        \"\"\"\n",
    "        mean_rewards = []\n",
    "        for t in range(len(rewards_array)):\n",
    "            mean_rewards.append(np.mean(rewards_array[max(0, t-100):(t+1)]))\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Mean Rewards')\n",
    "        plt.title(f'Mean Rewards vs Episodes (lr={self.alpha}, er={self.epsilon:0.2f})')\n",
    "        plt.savefig('cartpoleQLAgent.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
